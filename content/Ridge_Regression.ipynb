{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will include setting up the Ridge Regression and going through the optimization to determine the best weights using gradient descent. I plan to introduce this topic to my numerical class as we go through the linear regression chapter and letting them use the Ridge Regression for handwritting recognition and image recognition as a fun exercise for them to enjoy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The data used for this tutorial is from the UCI Machine Learning Repository. The dataset is for median value of homes in Boston that includes 13 features. The goal is to predict the value of each home given the 13 features. The features in the dataset are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. CRIM: per capita crime rate by town \n",
    "2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft. \n",
    "3. INDUS: proportion of non-retail business acres per town \n",
    "4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) \n",
    "5. NOX: nitric oxides concentration (parts per 10 million) \n",
    "6. RM: average number of rooms per dwelling \n",
    "7. AGE: proportion of owner-occupied units built prior to 1940 \n",
    "8. DIS: weighted distances to five Boston employment centres \n",
    "9. RAD: index of accessibility to radial highways \n",
    "10. TAX: full-value property-tax rate per \\$10,000 \n",
    "11. PTRATIO: pupil-teacher ratio by town \n",
    "12. B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town \n",
    "13. LSTAT: % lower status of the population \n",
    "14. MEDV: Median value of owner-occupied homes in \\$1000's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = load_boston().data\n",
    "y = load_boston().target\n",
    "\n",
    "load_boston().feature_names\n",
    "\n",
    "X = normalize(X, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ridge Regression is a linear regression with a l-2 penalty. The l-2 penalty allows us to deal with overfitting issues and penalize large values of $\\beta$ in our model. First, we will set up a cost/energy function that describes the \"cost\" to us, how much error our model has with the parameters $\\alpha$ and $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\displaystyle E(x) =  \\sum_{i = 1}^{n} (y_i - \\alpha - <\\beta, x_i>)^2 + \\lambda ||\\beta||^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to minimize this function by determining the best weights, $\\alpha$, $\\beta$. Since this energy function is convex, gradient descent will work just fine for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent is simply an iterative method that updates each parameter so that we take a step opposite of the direction of the gradient. This allows us to take a step \"down hill\" so we reach the minimum. First, we will find the gradient with respect to the parameters $\\alpha$ and $\\beta_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial \\alpha} &= -2\\sum_{i = 1}^{n} (y_i - \\alpha - <\\beta, x_i>) \\\\\n",
    "\\frac{\\partial E}{\\partial \\beta_j} &= -2 \\sum_{i = 1}^{n} (y_i - \\alpha - <\\beta, x_i>)x_{i}^j + 2\\lambda \\beta_j\n",
    "\\\\\n",
    "&= -2 X^{T}(\\vec{y} - \\vec{\\alpha} - X \\vec{\\beta}) + 2 \\lambda \\vec{\\beta}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the gradients, we simply update each parameter:\n",
    "$$\n",
    "\\alpha \\rightarrow \\alpha - \\eta \\frac{\\partial E}{\\partial \\alpha} \\\\\n",
    "\\beta_j \\rightarrow \\beta_j - \\eta \\frac{\\partial E}{\\partial \\beta_j}\n",
    "$$\n",
    "where $\\eta$ is the learning rate of the descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def E(X,y, alpha, beta, lam):\n",
    "    return np.sum(pow(y - np.repeat(alpha, len(X)) - np.dot(X,beta), 2)) + lam * np.linalg.norm(beta)**2\n",
    "\n",
    "def ridge(X, y, lam):\n",
    "    alpha = 0\n",
    "    beta = np.zeros(X.shape[1])\n",
    "    learning_rate = 0.001\n",
    "    max_iter = 10000\n",
    "    E_new = []\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        E_old = E(X, y, alpha, beta, lam)\n",
    "        \n",
    "        h = (np.repeat(alpha, len(X)) + np.dot(X, beta))\n",
    "        loss = y - h\n",
    "        cost = pow(h - y, 2)\n",
    "        \n",
    "        grad_alpha = -2.0 * np.sum(loss)\n",
    "        alpha = alpha - learning_rate * grad_alpha\n",
    "        \n",
    "        grad_beta = -2 * np.dot(X.T, loss) + 2 * lam * beta\n",
    "        beta = beta - learning_rate * grad_beta\n",
    "        \n",
    "        \n",
    "        E_new.append(E(X, y, alpha, beta, lam))\n",
    "        if abs(E_old - E_new[i]) < 1e-5:\n",
    "            print 'iteration: ', i\n",
    "            break\n",
    "    return alpha, beta, E_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  3009\n"
     ]
    }
   ],
   "source": [
    "alpha, beta, E = ridge(X,y,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.087474894848473"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-23.67539041,  23.35184911, -21.72476754,  17.63173486,\n",
       "        -6.75024633,  11.7497408 ,  -9.88680766,  -0.24042206,\n",
       "       -14.83054516, -14.39131326,  -7.63006426,   8.24639461, -44.82654007])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27416.928725946458"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((y - np.repeat(alpha, len(X)) - np.dot(X, beta))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~ttran0/234.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace = go.Scatter(\n",
    "    x = range(len(E)),\n",
    "    y = E\n",
    ")\n",
    "layout = go.Layout(\n",
    "    title = 'Energy Function Over Iterations',\n",
    "    xaxis = dict(title = 'iteration'),\n",
    "    yaxis = dict(title = 'Energy Function')\n",
    ")\n",
    "data = [trace]\n",
    "figure = Figure(data = data, layout = layout)\n",
    "py.iplot(figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we can see that the energy function decreases over the number of times we iterate through gradient descent, which is exactly what we want when we are minimizing a function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to make sure we did the gradient descent and found the parameters correctly, I will use scikit-learn's Ridge Regression model to compare our result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Ridge Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ridge_model = Ridge(alpha = 1.0)\n",
    "ridge_model.fit(X,y)\n",
    "\n",
    "beta_r =  ridge_model.coef_\n",
    "\n",
    "alpha_r = ridge_model.intercept_\n",
    "\n",
    "ridge_model.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.089811478652788"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-23.67274775,  23.35236303, -21.72914621,  17.62952319,\n",
       "        -6.75423947,  11.76105965,  -9.88461594,  -0.26512071,\n",
       "       -14.82209149, -14.3965544 ,  -7.64299595,   8.24769453, -44.85749936])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27413.730425497597"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((y - ridge_model.predict(X))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "We get the same results from our code and scikit-learn's model. This is a good indicator that we are on the right track and found the correct parameters. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
