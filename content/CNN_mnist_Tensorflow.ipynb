{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my last notebook, I used a multilayer perceptron network (MLP) for classification using the MNIST dataset. For this notebook, I will apply a Convolutional Neural Network to the same dataset and see how this network performs| compared to the MLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.misc import imread, imresize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.next_batch(10)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "We will be using the MNIST dataset. This dataset contains handwritten digits ranging from 0 - 9. Each image is 28x28 in resolution. Some examples of images in our set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1bee42c9fd0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE8G4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bee4281358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.next_batch(1)[0][0].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1bee432f828>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUIQfCIrwA0ERfiAowg8ERfiBoP4fEBXDaZVzWskAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bee42dfb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.next_batch(1)[0][0].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network Model (CNN)\n",
    "The CNN is similar to the multilayer percepton model we previously did with an addition of two types of layers: the convolution layer and the pooling layer. \n",
    "\n",
    "** Convolution Layer** :\n",
    "Given an image, the convolution layer consists of a matrix (filter), often a matrix smaller than the image, that is multiplied element wise with the image matrix in strides. With each stride, the element wise multiplication is then summed and stored into a new matrix. For example, lets consider the image matrix \n",
    "\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 0 & 0 \\\\\n",
    "0 & 1 & 1 & 1 & 0 \\\\\n",
    "0 & 0 & 1 & 1 & 1 \\\\\n",
    "0 & 0 & 1 & 1 & 0 \\\\\n",
    "0 & 1 & 1 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\\text{Image}\n",
    "$$\n",
    "and we apply this filter matrix to the image with a stride of 1.\n",
    "\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$ \\text{Filter}$$\n",
    "\n",
    "Then the convolved feature matrix will be:\n",
    "![title](http://engineering.flipboard.com/assets/convnets/Convolution_schematic.gif)\n",
    "\n",
    "The stride is simply how many pixels we slide our filter matrix over to perform the element wise multiplication and sum. For our example, we used a stride of 1 so notice the filter matrix moves over just one pixel between each operation. \n",
    "\n",
    "The filter matrix's purpose is to determine distinctive features of the images we are trying to classify such as edges. The filter is learned when we train our model and at each convolution layer, we can apply multiple filters.\n",
    "\n",
    "** Pooling Layer **:\n",
    "The pooling layer is a way to do non-linear down sampling. Max pooling is the most common type of pooling. The max pooling layer partitions the input matrix into non-overlapping sub-regions in a defined number of strides and computes the max of the sub-region and stores the max into a result matrix. An example of max pooling with stride = 2:\n",
    "\n",
    "![title](https://upload.wikimedia.org/wikipedia/commons/e/e9/Max_pooling.png)\n",
    "\n",
    "Sources of images:\n",
    "- http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution\n",
    "\n",
    "- http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "Both the convolution and pooling layers are ways to not only detect distinctive features in our classes, but they also eliminate the number of parameters in our model. This is a huge deal because using a traditional neural network for image classification can be very costly in terms of the number of parameters required. \n",
    "\n",
    "** Fun fact:** The structure of a convolutional neural network is motivated by the animal visual cortex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of our network will be:\n",
    "\n",
    "[Convolution --> Convolution --> Max Pooling --> Convolution --> Convolution --> Max Pooling --> Fully Connected --> Fully Connected --> Output]\n",
    "\n",
    "At each of our convolution layers, we will apply 64 3x3 filters with stride 1. Our max pooling layer will be a max pooling layer of size 2x2 with stride 2.\n",
    "The 2 fully connected layers will be like our perceptron model, which will contain 512 neurons each. The output layer will have 10 neurons because we have 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape = [None, 28 * 28])\n",
    "y = tf.placeholder(tf.float32, shape = [None, 10])\n",
    "\n",
    "def weight_var(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_var(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "\n",
    "def max_pool(x):\n",
    "    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "#Convolution Layer\n",
    "W_conv1 = weight_var([3, 3, 1, 64])\n",
    "b_conv1 = bias_var([64])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "#Convolution Layer\n",
    "W_conv2 = weight_var([3, 3, 64, 64])\n",
    "b_conv2 = bias_var([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2) + b_conv2)\n",
    "#Max Pool Layer\n",
    "h_pool1 = max_pool(h_conv2)\n",
    "\n",
    "\n",
    "#Convolution Layer\n",
    "W_conv3 = weight_var([3, 3, 64, 64])\n",
    "b_conv3 = bias_var([64])\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool1, W_conv3) + b_conv3)\n",
    "\n",
    "#Convolution Layer\n",
    "W_conv4 = weight_var([3, 3, 64, 64])\n",
    "b_conv4 = bias_var([64])\n",
    "\n",
    "h_conv4 = tf.nn.relu(conv2d(h_conv3, W_conv4) + b_conv4)\n",
    "\n",
    "#Max Pool Layer\n",
    "h_pool2 = max_pool(h_conv4)\n",
    "\n",
    "#Full Layer\n",
    "W_fc1 = weight_var([7 * 7 * 64, 512])\n",
    "b_fc1 = bias_var([512])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2,[-1, 7 * 7 * 64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "#Dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "#Full Layer\n",
    "W_fc2 = weight_var([512, 10])\n",
    "b_fc2 = bias_var([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our output layer, we will use the cross entropy function to produce our logits and will use the ADAM stochastic optimization to train our model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = y_conv))\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "correct_pred = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y, 1))\n",
    "acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy: 0.09\n",
      "test_accuracy: 0.99\n",
      "test_accuracy: 0.99\n",
      "test_accuracy: 1\n",
      "test_accuracy: 1\n",
      "test_accuracy: 0.99\n",
      "test_accuracy: 0.99\n",
      "test_accuracy: 1\n",
      "test_accuracy: 1\n",
      "test_accuracy: 0.99\n",
      "test_accuracy: 1\n",
      "test_accuracy: 1\n",
      "test_accuracy: 1\n",
      "test_accuracy: 1\n",
      "test_accuracy: 0.99\n",
      "test_accuracy: 1\n",
      "test_accuracy: 1\n",
      "test_accuracy: 1\n",
      "test_accuracy: 1\n",
      "test_accuracy: 0.99\n",
      "Wall time: 5min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(20000):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    test_batch = mnist.test.next_batch(100)\n",
    "    if i % 1000 == 0:\n",
    "#         train_accuracy = acc.eval(feed_dict = {x:batch[0], y:batch[1], keep_prob: 1.0})\n",
    "        test_accuracy = acc.eval(feed_dict = {x:test_batch[0], y:test_batch[1], keep_prob: 1.0})\n",
    "        print('test_accuracy: %g' %(test_accuracy))\n",
    "    train_step.run(feed_dict = {x: batch[0], y: batch[1], keep_prob: 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average test accuracy 99.34\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = 0\n",
    "for i in range(100):\n",
    "    test_set = mnist.test.next_batch(100)\n",
    "    test_accuracy += acc.eval(feed_dict = {x:test_set[0], y:test_set[1], keep_prob: 1.0})  \n",
    "avg_test_acc = test_accuracy/100 * 100\n",
    "print('average test accuracy %g'%avg_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can see that we get much better results from this model than using the multilayer perceptron network (MLP). The MLP network achieved a 96.7% accuracy while our CNN network best it at 99.34%. Training these two networks have been enjoyable on a GPU. The training times on a GPU vs CPU are drastic. The training for the CNN only took a little more than 5 minutes. While it would take more than 30 mins to train on my quad core Macbook Pro CPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
